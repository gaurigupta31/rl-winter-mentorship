# Reinforcement-Learning

Week 0
Before the start of the course, we except you to have completed all the administration work and prerequisites. Also, some pop-sciency knowledge never hurt anyone:

- [x] [TextBook] Chaper 1 from SnB: The Reinforcement Learning Problem

- [x] [Intuitive] What is reinforcement learning?

- [x] [Lecture] David Silver Lecture 1

- [x] Wikipedia article on RL

Week 1
Mathematical foundation of RL - Markov Decision Processes.

[TextBook] Chapter 3 from SnB: Finite Markov Decision Processes

[Intuitive] Reinforcement Learning Demystified: Markov Decision Processes

[Lecture] David Silver Lecture 2

Deliverables:

Solve Excercises 3.1, 3.2 and 3.3 from Sna (Page: 85). Write a report using Markdown or Google Docs. Please keep the answers as brief as possible, you won't be assessed on the length of the report.
Week 2
Tabular methods

[TextBook] Chapter 4 from SnB: Dynamic Programming

[Intuitive, Code] Medium article

[Lecture] David Silver Lecture 3

Deliverables:

Solve all environments from gym-gridworlds using value and policy iteration using only NumPy.
Week 3
Function Approximators

[TextBook] Chapter 9, 10, 11 from SnB
[Lecture] David Silver: Lecture 6, Lecture 7
[Code] OpenAI Spinning Up
Deliverables:

Build a single layer neural network using only NumPy to solve CartPole using:

Q-Learning
Vanilla Policy gradients
Week 4
Competition Week

We'll give you some data from an expert controlling a robot. Your task to create the best agent either using the expert data or not. Throughout the week, we'll maintain a leaderboard of scores, and each mentee can have multiple submissions. The format for submissions will be announced later. (#TODO)

Week 5
Intro to Deep RL. The leap from the previous week to this will be quite substantial. The exact specifics of this week is open-ended, it's entirely up to the mentee to decide what they want to pursue. A few potential options are:

Reimplementing a seminal research paper like DQN, PPO, etc.

Using an existing library on new unexplored environments like your favourite FPS game, or in more unconvential problems like solving symbolic integration.
